name: ML Pipeline - Training & Deployment

on:
  push:
    branches: [main, develop]
    paths:
      - "src/**"
      - "configs/**"
      - "data/**"
      - "requirements.txt"
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      force_retrain:
        description: "Force model retraining"
        required: false
        default: "false"
        type: boolean

env:
  PYTHON_VERSION: "3.11"

jobs:
  data-validation:
    runs-on: ubuntu-latest
    outputs:
      data-changed: ${{ steps.check-data.outputs.changed }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Check if data files changed
        id: check-data
        run: |
          # For testing, always run the pipeline
          if git diff --name-only HEAD^ HEAD | grep -E "(data/|configs/|src/)" || [ "${{ github.event.inputs.force_retrain }}" == "true" ] || [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "changed=true" >> $GITHUB_OUTPUT
          else
            echo "changed=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check-data.outputs.changed == 'true'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        if: steps.check-data.outputs.changed == 'true'
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Validate data integrity
        if: steps.check-data.outputs.changed == 'true'
        run: |
          python -c "
          import pandas as pd
          import sys

          # Check if raw data exists and is valid
          try:
              data = pd.read_csv('data/raw/house_data.csv')
              print(f'Raw data loaded: {len(data)} rows, {len(data.columns)} columns')
              
              # Basic data quality checks - adjusted for smaller dataset
              if len(data) < 50:  # Reduced from 100 to 50
                  print('ERROR: Insufficient data samples (minimum 50 required)')
                  sys.exit(1)
              
              if data.isnull().sum().sum() > len(data) * 0.7:  # Allow more missing values
                  print('ERROR: Too many missing values (>70%)')
                  sys.exit(1)
                  
              print(f'✅ Data validation passed: {len(data)} samples with {len(data.columns)} features')
          except Exception as e:
              print(f'ERROR: Data validation failed: {e}')
              sys.exit(1)
          "

  ml-pipeline:
    needs: data-validation
    if: needs.data-validation.outputs.data-changed == 'true'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

          # Ensure MLflow and Databricks packages are installed for CI
          pip install mlflow==2.3.1 databricks-cli==0.18.0 databricks-sdk==0.12.0

      - name: Configure Databricks MLflow connection
        env:
          DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          MLFLOW_TRACKING_URI: ${{ vars.MLFLOW_TRACKING_URI }}
        run: |
          echo "Configuring Databricks MLflow connection..."

          # Set up Databricks CLI configuration
          mkdir -p ~/.databrickscfg
          cat > ~/.databrickscfg << EOF
          [DEFAULT]
          host = $DATABRICKS_HOST
          token = $DATABRICKS_TOKEN
          EOF

          echo "✅ Databricks configuration created"

      - name: Verify Databricks MLflow connection
        env:
          DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          MLFLOW_TRACKING_URI: ${{ vars.MLFLOW_TRACKING_URI }}
        run: |
          echo "Testing connection to Databricks MLflow at: $MLFLOW_TRACKING_URI"

          # Test MLflow connection with Databricks authentication
          python -c "
          import mlflow
          import os

          # Set environment variables for Databricks authentication
          os.environ['DATABRICKS_HOST'] = os.getenv('DATABRICKS_HOST')
          os.environ['DATABRICKS_TOKEN'] = os.getenv('DATABRICKS_TOKEN')

          mlflow_uri = os.getenv('MLFLOW_TRACKING_URI')
          if not mlflow_uri:
              print('❌ MLFLOW_TRACKING_URI not set')
              exit(1)

          mlflow.set_tracking_uri(mlflow_uri)
          try:
              # Try to list experiments to test connection
              experiments = mlflow.search_experiments()
              print(f'✅ Successfully connected to Databricks MLflow at {mlflow_uri}')
              print(f'Found {len(experiments)} experiments')
          except Exception as e:
              print(f'❌ Failed to connect to Databricks MLflow: {e}')
              print('Check your DATABRICKS_HOST and DATABRICKS_TOKEN')
              exit(1)
          "

      - name: Create necessary directories
        run: |
          mkdir -p data/processed
          mkdir -p models/trained
          mkdir -p logs

      - name: Run ML Pipeline
        id: pipeline
        env:
          MLFLOW_TRACKING_URI: ${{ vars.MLFLOW_TRACKING_URI }}
        run: |
          python src/pipeline/orchestrator.py \
            --config configs/model_config.yaml \
            --mlflow-uri "${MLFLOW_TRACKING_URI}" \
            2>&1 | tee pipeline.log

          # Check if pipeline succeeded
          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "pipeline_success=true" >> $GITHUB_OUTPUT
          else
            echo "pipeline_success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload Pipeline Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs
          path: |
            pipeline.log
            logs/

      - name: Upload Model Artifacts
        if: steps.pipeline.outputs.pipeline_success == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            models/trained/
            configs/model_config.yaml

      - name: Test API Integration
        if: steps.pipeline.outputs.pipeline_success == 'true'
        run: |
          # Start the API in background
          cd src/api
          python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
          API_PID=$!
          sleep 10

          # Test health endpoint
          curl -f http://localhost:8000/health || exit 1

          # Test prediction endpoint
          curl -X POST "http://localhost:8000/predict" \
            -H "Content-Type: application/json" \
            -d '{
              "sqft": 1500,
              "bedrooms": 3,
              "bathrooms": 2,
              "location": "suburban",
              "year_built": 2000,
              "condition": "fair"
            }' || exit 1

          # Clean up
          kill $API_PID

  build-and-push:
    needs: ml-pipeline
    if: success() && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: ./

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'eu-west-1' }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push API image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY_API: ${{ vars.ECR_REPOSITORY_API || 'house-price-api' }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build API image
          docker build -f Dockerfile -t $ECR_REGISTRY/$ECR_REPOSITORY_API:$IMAGE_TAG .
          docker build -f Dockerfile -t $ECR_REGISTRY/$ECR_REPOSITORY_API:latest .

          # Push API image
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_API:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_API:latest

          echo "API image pushed: $ECR_REGISTRY/$ECR_REPOSITORY_API:$IMAGE_TAG"

      - name: Build and push UI image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY_UI: ${{ vars.ECR_REPOSITORY_UI || 'house-price-ui' }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build UI image
          docker build -f streamlit_app/Dockerfile -t $ECR_REGISTRY/$ECR_REPOSITORY_UI:$IMAGE_TAG ./streamlit_app
          docker build -f streamlit_app/Dockerfile -t $ECR_REGISTRY/$ECR_REPOSITORY_UI:latest ./streamlit_app

          # Push UI image
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_UI:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_UI:latest

          echo "UI image pushed: $ECR_REGISTRY/$ECR_REPOSITORY_UI:$IMAGE_TAG"

  notify:
    needs: [data-validation, ml-pipeline, build-and-push]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Notify on success
        if: needs.ml-pipeline.result == 'success' && needs.build-and-push.result == 'success'
        run: |
          echo "✅ ML Pipeline completed successfully!"
          echo "Model trained and validated"
          echo "Images built and pushed to ECR"

      - name: Notify on pipeline failure
        if: needs.ml-pipeline.result == 'failure'
        run: |
          echo "❌ ML Pipeline failed!"
          echo "Check the logs for details"
          exit 1

      - name: Notify on build failure
        if: needs.build-and-push.result == 'failure'
        run: |
          echo "❌ Image build/push failed!"
          echo "Check the build logs for details"
          exit 1
